{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2b7870b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amirel/.pyenv/versions/3.8.12/envs/time_series/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-09-09 15:41:26.368408: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-09 15:41:26.770293: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-09-09 15:41:26.797097: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-09 15:41:26.797114: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-09-09 15:41:26.858367: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-09 15:41:27.541982: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-09 15:41:27.542159: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-09 15:41:27.542166: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.models import RNNModel, ExponentialSmoothing, BlockRNNModel\n",
    "from darts.metrics import mape\n",
    "from darts.utils.statistics import check_seasonality, plot_acf\n",
    "from darts.datasets import AirPassengersDataset, SunspotsDataset\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow import expand_dims\n",
    "from tensorflow.keras.losses import MeanAbsolutePercentageError\n",
    "import tensorflow\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "from numpy import asarray\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c673e0",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c23b009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../AltInt/Inflation_prediction/raw_data/final_df.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bc005fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brent_Crude_GBP</th>\n",
       "      <th>WTI_Crude_GBP</th>\n",
       "      <th>GBP Curncy</th>\n",
       "      <th>GBPEUR Curncy</th>\n",
       "      <th>UKX Index</th>\n",
       "      <th>SPX Index</th>\n",
       "      <th>SX5E Index</th>\n",
       "      <th>Natural_Gas</th>\n",
       "      <th>CPI</th>\n",
       "      <th>RPI</th>\n",
       "      <th>GBP_IRS_2y</th>\n",
       "      <th>GBP_IRS_10y</th>\n",
       "      <th>Nationwide_HPI</th>\n",
       "      <th>RPI YOY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-01-01</th>\n",
       "      <td>30.903838</td>\n",
       "      <td>33.424333</td>\n",
       "      <td>1.479027</td>\n",
       "      <td>1.099905</td>\n",
       "      <td>4308.390000</td>\n",
       "      <td>596.071497</td>\n",
       "      <td>2135.982324</td>\n",
       "      <td>12.472805</td>\n",
       "      <td>84.9</td>\n",
       "      <td>210.1</td>\n",
       "      <td>2.122545</td>\n",
       "      <td>3.757500</td>\n",
       "      <td>300.2</td>\n",
       "      <td>0.142993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-02-01</th>\n",
       "      <td>29.393166</td>\n",
       "      <td>33.617553</td>\n",
       "      <td>1.492643</td>\n",
       "      <td>1.120824</td>\n",
       "      <td>4232.114762</td>\n",
       "      <td>570.410394</td>\n",
       "      <td>2020.716905</td>\n",
       "      <td>9.467866</td>\n",
       "      <td>85.6</td>\n",
       "      <td>211.4</td>\n",
       "      <td>2.129905</td>\n",
       "      <td>3.798333</td>\n",
       "      <td>294.7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-03-01</th>\n",
       "      <td>31.733352</td>\n",
       "      <td>36.826152</td>\n",
       "      <td>1.494241</td>\n",
       "      <td>1.097877</td>\n",
       "      <td>4107.840909</td>\n",
       "      <td>566.393975</td>\n",
       "      <td>2029.160819</td>\n",
       "      <td>8.064295</td>\n",
       "      <td>85.8</td>\n",
       "      <td>211.3</td>\n",
       "      <td>2.142045</td>\n",
       "      <td>3.872500</td>\n",
       "      <td>301.1</td>\n",
       "      <td>-0.377181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-04-01</th>\n",
       "      <td>33.990786</td>\n",
       "      <td>36.012519</td>\n",
       "      <td>1.511936</td>\n",
       "      <td>1.125327</td>\n",
       "      <td>4259.890500</td>\n",
       "      <td>588.042125</td>\n",
       "      <td>2119.383029</td>\n",
       "      <td>7.762397</td>\n",
       "      <td>86.0</td>\n",
       "      <td>211.5</td>\n",
       "      <td>2.143250</td>\n",
       "      <td>3.935500</td>\n",
       "      <td>302.9</td>\n",
       "      <td>-1.168224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-01</th>\n",
       "      <td>37.524515</td>\n",
       "      <td>38.801542</td>\n",
       "      <td>1.561252</td>\n",
       "      <td>1.131686</td>\n",
       "      <td>4434.719500</td>\n",
       "      <td>582.718727</td>\n",
       "      <td>2164.120107</td>\n",
       "      <td>7.757394</td>\n",
       "      <td>86.4</td>\n",
       "      <td>212.8</td>\n",
       "      <td>2.031900</td>\n",
       "      <td>3.917500</td>\n",
       "      <td>307.2</td>\n",
       "      <td>-1.069270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-01</th>\n",
       "      <td>89.748267</td>\n",
       "      <td>85.128666</td>\n",
       "      <td>1.247484</td>\n",
       "      <td>1.178074</td>\n",
       "      <td>7472.767778</td>\n",
       "      <td>3274.256696</td>\n",
       "      <td>3179.554466</td>\n",
       "      <td>81.073170</td>\n",
       "      <td>120.8</td>\n",
       "      <td>337.1</td>\n",
       "      <td>2.538121</td>\n",
       "      <td>2.240542</td>\n",
       "      <td>538.4</td>\n",
       "      <td>11.659490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-01</th>\n",
       "      <td>95.251442</td>\n",
       "      <td>87.836565</td>\n",
       "      <td>1.233611</td>\n",
       "      <td>1.167758</td>\n",
       "      <td>7238.188421</td>\n",
       "      <td>3172.340166</td>\n",
       "      <td>3058.969505</td>\n",
       "      <td>96.746393</td>\n",
       "      <td>121.8</td>\n",
       "      <td>340.0</td>\n",
       "      <td>2.933563</td>\n",
       "      <td>2.626558</td>\n",
       "      <td>541.8</td>\n",
       "      <td>11.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-01</th>\n",
       "      <td>85.363871</td>\n",
       "      <td>80.415504</td>\n",
       "      <td>1.231479</td>\n",
       "      <td>1.184589</td>\n",
       "      <td>7303.865263</td>\n",
       "      <td>3298.689210</td>\n",
       "      <td>3082.959231</td>\n",
       "      <td>128.688654</td>\n",
       "      <td>122.5</td>\n",
       "      <td>343.2</td>\n",
       "      <td>2.700779</td>\n",
       "      <td>2.302258</td>\n",
       "      <td>541.0</td>\n",
       "      <td>12.340426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-01</th>\n",
       "      <td>80.656541</td>\n",
       "      <td>78.913331</td>\n",
       "      <td>1.211816</td>\n",
       "      <td>1.181732</td>\n",
       "      <td>7453.987222</td>\n",
       "      <td>3446.779300</td>\n",
       "      <td>3131.457852</td>\n",
       "      <td>180.082147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.368889</td>\n",
       "      <td>2.688500</td>\n",
       "      <td>546.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-01</th>\n",
       "      <td>73.474167</td>\n",
       "      <td>80.706056</td>\n",
       "      <td>1.272420</td>\n",
       "      <td>1.180400</td>\n",
       "      <td>7403.016000</td>\n",
       "      <td>3300.260920</td>\n",
       "      <td>3208.537784</td>\n",
       "      <td>93.341821</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.475340</td>\n",
       "      <td>2.155940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Brent_Crude_GBP  WTI_Crude_GBP  GBP Curncy  GBPEUR Curncy  \\\n",
       "Date                                                                    \n",
       "2009-01-01        30.903838      33.424333    1.479027       1.099905   \n",
       "2009-02-01        29.393166      33.617553    1.492643       1.120824   \n",
       "2009-03-01        31.733352      36.826152    1.494241       1.097877   \n",
       "2009-04-01        33.990786      36.012519    1.511936       1.125327   \n",
       "2009-05-01        37.524515      38.801542    1.561252       1.131686   \n",
       "...                     ...            ...         ...            ...   \n",
       "2022-05-01        89.748267      85.128666    1.247484       1.178074   \n",
       "2022-06-01        95.251442      87.836565    1.233611       1.167758   \n",
       "2022-07-01        85.363871      80.415504    1.231479       1.184589   \n",
       "2022-08-01        80.656541      78.913331    1.211816       1.181732   \n",
       "2022-09-01        73.474167      80.706056    1.272420       1.180400   \n",
       "\n",
       "              UKX Index    SPX Index   SX5E Index  Natural_Gas    CPI    RPI  \\\n",
       "Date                                                                           \n",
       "2009-01-01  4308.390000   596.071497  2135.982324    12.472805   84.9  210.1   \n",
       "2009-02-01  4232.114762   570.410394  2020.716905     9.467866   85.6  211.4   \n",
       "2009-03-01  4107.840909   566.393975  2029.160819     8.064295   85.8  211.3   \n",
       "2009-04-01  4259.890500   588.042125  2119.383029     7.762397   86.0  211.5   \n",
       "2009-05-01  4434.719500   582.718727  2164.120107     7.757394   86.4  212.8   \n",
       "...                 ...          ...          ...          ...    ...    ...   \n",
       "2022-05-01  7472.767778  3274.256696  3179.554466    81.073170  120.8  337.1   \n",
       "2022-06-01  7238.188421  3172.340166  3058.969505    96.746393  121.8  340.0   \n",
       "2022-07-01  7303.865263  3298.689210  3082.959231   128.688654  122.5  343.2   \n",
       "2022-08-01  7453.987222  3446.779300  3131.457852   180.082147    NaN    NaN   \n",
       "2022-09-01  7403.016000  3300.260920  3208.537784    93.341821    NaN    NaN   \n",
       "\n",
       "            GBP_IRS_2y  GBP_IRS_10y  Nationwide_HPI    RPI YOY  \n",
       "Date                                                            \n",
       "2009-01-01    2.122545     3.757500           300.2   0.142993  \n",
       "2009-02-01    2.129905     3.798333           294.7   0.000000  \n",
       "2009-03-01    2.142045     3.872500           301.1  -0.377181  \n",
       "2009-04-01    2.143250     3.935500           302.9  -1.168224  \n",
       "2009-05-01    2.031900     3.917500           307.2  -1.069270  \n",
       "...                ...          ...             ...        ...  \n",
       "2022-05-01    2.538121     2.240542           538.4  11.659490  \n",
       "2022-06-01    2.933563     2.626558           541.8  11.842105  \n",
       "2022-07-01    2.700779     2.302258           541.0  12.340426  \n",
       "2022-08-01    3.368889     2.688500           546.1        NaN  \n",
       "2022-09-01    2.475340     2.155940             NaN        NaN  \n",
       "\n",
       "[165 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index = pd.to_datetime(df.index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fef98609",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = ['RPI', 'CPI', 'RPI YOY'])\n",
    "y = df['RPI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90379fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brent_Crude_GBP</th>\n",
       "      <th>WTI_Crude_GBP</th>\n",
       "      <th>GBP Curncy</th>\n",
       "      <th>GBPEUR Curncy</th>\n",
       "      <th>UKX Index</th>\n",
       "      <th>SPX Index</th>\n",
       "      <th>SX5E Index</th>\n",
       "      <th>Natural_Gas</th>\n",
       "      <th>GBP_IRS_2y</th>\n",
       "      <th>GBP_IRS_10y</th>\n",
       "      <th>Nationwide_HPI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01</th>\n",
       "      <td>32.818125</td>\n",
       "      <td>31.737054</td>\n",
       "      <td>1.516177</td>\n",
       "      <td>1.336264</td>\n",
       "      <td>6641.765238</td>\n",
       "      <td>1339.503656</td>\n",
       "      <td>2495.664852</td>\n",
       "      <td>13.099203</td>\n",
       "      <td>0.858274</td>\n",
       "      <td>1.684729</td>\n",
       "      <td>375.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-01</th>\n",
       "      <td>38.343255</td>\n",
       "      <td>32.759207</td>\n",
       "      <td>1.533386</td>\n",
       "      <td>1.362371</td>\n",
       "      <td>6746.840476</td>\n",
       "      <td>1356.232800</td>\n",
       "      <td>2536.801725</td>\n",
       "      <td>13.938965</td>\n",
       "      <td>0.952887</td>\n",
       "      <td>1.856450</td>\n",
       "      <td>375.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-01</th>\n",
       "      <td>37.866261</td>\n",
       "      <td>31.635113</td>\n",
       "      <td>1.503677</td>\n",
       "      <td>1.377568</td>\n",
       "      <td>6791.140000</td>\n",
       "      <td>1380.431518</td>\n",
       "      <td>2615.694574</td>\n",
       "      <td>13.821672</td>\n",
       "      <td>0.947216</td>\n",
       "      <td>1.783045</td>\n",
       "      <td>377.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-01</th>\n",
       "      <td>40.409377</td>\n",
       "      <td>35.572047</td>\n",
       "      <td>1.512909</td>\n",
       "      <td>1.386036</td>\n",
       "      <td>6886.650952</td>\n",
       "      <td>1381.813784</td>\n",
       "      <td>2620.084413</td>\n",
       "      <td>13.690119</td>\n",
       "      <td>0.923663</td>\n",
       "      <td>1.811432</td>\n",
       "      <td>385.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-01</th>\n",
       "      <td>42.505052</td>\n",
       "      <td>36.186768</td>\n",
       "      <td>1.543548</td>\n",
       "      <td>1.384014</td>\n",
       "      <td>6864.994500</td>\n",
       "      <td>1360.216539</td>\n",
       "      <td>2568.111738</td>\n",
       "      <td>13.123988</td>\n",
       "      <td>1.009908</td>\n",
       "      <td>1.971630</td>\n",
       "      <td>389.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-01</th>\n",
       "      <td>41.067582</td>\n",
       "      <td>35.885473</td>\n",
       "      <td>1.552386</td>\n",
       "      <td>1.385524</td>\n",
       "      <td>6687.803500</td>\n",
       "      <td>1340.919813</td>\n",
       "      <td>2507.691659</td>\n",
       "      <td>13.023503</td>\n",
       "      <td>1.061182</td>\n",
       "      <td>2.089545</td>\n",
       "      <td>389.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-01</th>\n",
       "      <td>36.726002</td>\n",
       "      <td>31.989864</td>\n",
       "      <td>1.545617</td>\n",
       "      <td>1.402913</td>\n",
       "      <td>6625.526087</td>\n",
       "      <td>1353.218881</td>\n",
       "      <td>2518.626151</td>\n",
       "      <td>13.169073</td>\n",
       "      <td>1.119989</td>\n",
       "      <td>2.051104</td>\n",
       "      <td>390.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-08-01</th>\n",
       "      <td>31.196110</td>\n",
       "      <td>29.152671</td>\n",
       "      <td>1.545248</td>\n",
       "      <td>1.376957</td>\n",
       "      <td>6402.255000</td>\n",
       "      <td>1311.027051</td>\n",
       "      <td>2434.955959</td>\n",
       "      <td>12.478583</td>\n",
       "      <td>1.085050</td>\n",
       "      <td>1.958367</td>\n",
       "      <td>389.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-09-01</th>\n",
       "      <td>31.774855</td>\n",
       "      <td>30.721787</td>\n",
       "      <td>1.527609</td>\n",
       "      <td>1.364657</td>\n",
       "      <td>6280.524348</td>\n",
       "      <td>1302.101893</td>\n",
       "      <td>2375.712634</td>\n",
       "      <td>12.595702</td>\n",
       "      <td>1.004023</td>\n",
       "      <td>1.908904</td>\n",
       "      <td>390.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-01</th>\n",
       "      <td>32.150956</td>\n",
       "      <td>30.493892</td>\n",
       "      <td>1.533165</td>\n",
       "      <td>1.378817</td>\n",
       "      <td>6458.806957</td>\n",
       "      <td>1339.915435</td>\n",
       "      <td>2458.240837</td>\n",
       "      <td>12.291281</td>\n",
       "      <td>0.940307</td>\n",
       "      <td>1.899183</td>\n",
       "      <td>392.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-01</th>\n",
       "      <td>30.159048</td>\n",
       "      <td>28.998764</td>\n",
       "      <td>1.523005</td>\n",
       "      <td>1.409255</td>\n",
       "      <td>6398.511500</td>\n",
       "      <td>1357.124134</td>\n",
       "      <td>2446.822967</td>\n",
       "      <td>12.095331</td>\n",
       "      <td>0.994821</td>\n",
       "      <td>1.897445</td>\n",
       "      <td>391.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-01</th>\n",
       "      <td>25.818625</td>\n",
       "      <td>27.520854</td>\n",
       "      <td>1.506823</td>\n",
       "      <td>1.366814</td>\n",
       "      <td>6324.143500</td>\n",
       "      <td>1363.285466</td>\n",
       "      <td>2426.843201</td>\n",
       "      <td>11.423042</td>\n",
       "      <td>1.016488</td>\n",
       "      <td>1.930550</td>\n",
       "      <td>393.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>22.809737</td>\n",
       "      <td>25.534394</td>\n",
       "      <td>1.399643</td>\n",
       "      <td>1.278787</td>\n",
       "      <td>6123.662727</td>\n",
       "      <td>1413.405854</td>\n",
       "      <td>2340.610930</td>\n",
       "      <td>9.773593</td>\n",
       "      <td>0.898800</td>\n",
       "      <td>1.504270</td>\n",
       "      <td>392.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-01</th>\n",
       "      <td>24.047857</td>\n",
       "      <td>26.087339</td>\n",
       "      <td>1.394184</td>\n",
       "      <td>1.260416</td>\n",
       "      <td>6171.272222</td>\n",
       "      <td>1434.117439</td>\n",
       "      <td>2327.359810</td>\n",
       "      <td>9.383225</td>\n",
       "      <td>0.757655</td>\n",
       "      <td>1.332758</td>\n",
       "      <td>392.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-01</th>\n",
       "      <td>28.183023</td>\n",
       "      <td>28.331057</td>\n",
       "      <td>1.411843</td>\n",
       "      <td>1.258433</td>\n",
       "      <td>6253.627368</td>\n",
       "      <td>1452.882656</td>\n",
       "      <td>2395.269808</td>\n",
       "      <td>9.068961</td>\n",
       "      <td>0.805940</td>\n",
       "      <td>1.375611</td>\n",
       "      <td>399.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-01</th>\n",
       "      <td>30.652665</td>\n",
       "      <td>29.521709</td>\n",
       "      <td>1.413891</td>\n",
       "      <td>1.256627</td>\n",
       "      <td>6357.307727</td>\n",
       "      <td>1468.004077</td>\n",
       "      <td>2426.290431</td>\n",
       "      <td>9.156711</td>\n",
       "      <td>0.820131</td>\n",
       "      <td>1.418518</td>\n",
       "      <td>403.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-01</th>\n",
       "      <td>33.610150</td>\n",
       "      <td>32.363833</td>\n",
       "      <td>1.417632</td>\n",
       "      <td>1.269523</td>\n",
       "      <td>6305.287143</td>\n",
       "      <td>1461.227431</td>\n",
       "      <td>2356.760057</td>\n",
       "      <td>9.512007</td>\n",
       "      <td>0.805012</td>\n",
       "      <td>1.354362</td>\n",
       "      <td>407.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-01</th>\n",
       "      <td>35.853479</td>\n",
       "      <td>33.841453</td>\n",
       "      <td>1.392536</td>\n",
       "      <td>1.244714</td>\n",
       "      <td>6262.087727</td>\n",
       "      <td>1497.344936</td>\n",
       "      <td>2335.194660</td>\n",
       "      <td>10.313098</td>\n",
       "      <td>0.707955</td>\n",
       "      <td>1.206173</td>\n",
       "      <td>408.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-01</th>\n",
       "      <td>34.979696</td>\n",
       "      <td>32.963639</td>\n",
       "      <td>1.330336</td>\n",
       "      <td>1.201241</td>\n",
       "      <td>6630.369545</td>\n",
       "      <td>1608.590787</td>\n",
       "      <td>2481.757122</td>\n",
       "      <td>10.837997</td>\n",
       "      <td>0.501131</td>\n",
       "      <td>1.021027</td>\n",
       "      <td>410.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-01</th>\n",
       "      <td>35.390659</td>\n",
       "      <td>33.703121</td>\n",
       "      <td>1.332530</td>\n",
       "      <td>1.191674</td>\n",
       "      <td>6658.116818</td>\n",
       "      <td>1603.181916</td>\n",
       "      <td>2513.310834</td>\n",
       "      <td>9.386618</td>\n",
       "      <td>0.425193</td>\n",
       "      <td>0.936239</td>\n",
       "      <td>411.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-01</th>\n",
       "      <td>35.650483</td>\n",
       "      <td>33.536749</td>\n",
       "      <td>1.325100</td>\n",
       "      <td>1.186100</td>\n",
       "      <td>6681.491364</td>\n",
       "      <td>1607.695474</td>\n",
       "      <td>2522.149367</td>\n",
       "      <td>10.132168</td>\n",
       "      <td>0.440250</td>\n",
       "      <td>0.959009</td>\n",
       "      <td>411.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-01</th>\n",
       "      <td>40.400342</td>\n",
       "      <td>37.170194</td>\n",
       "      <td>1.271960</td>\n",
       "      <td>1.152440</td>\n",
       "      <td>6784.034000</td>\n",
       "      <td>1663.042077</td>\n",
       "      <td>2624.813005</td>\n",
       "      <td>12.497445</td>\n",
       "      <td>0.557524</td>\n",
       "      <td>1.138360</td>\n",
       "      <td>410.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-01</th>\n",
       "      <td>36.390691</td>\n",
       "      <td>33.946930</td>\n",
       "      <td>1.293700</td>\n",
       "      <td>1.192923</td>\n",
       "      <td>6618.945455</td>\n",
       "      <td>1649.769763</td>\n",
       "      <td>2519.838212</td>\n",
       "      <td>12.435263</td>\n",
       "      <td>0.646977</td>\n",
       "      <td>1.316939</td>\n",
       "      <td>408.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01</th>\n",
       "      <td>42.778165</td>\n",
       "      <td>38.042859</td>\n",
       "      <td>1.283743</td>\n",
       "      <td>1.197404</td>\n",
       "      <td>6796.544286</td>\n",
       "      <td>1706.510219</td>\n",
       "      <td>2631.646533</td>\n",
       "      <td>12.452421</td>\n",
       "      <td>0.649512</td>\n",
       "      <td>1.293796</td>\n",
       "      <td>410.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Brent_Crude_GBP  WTI_Crude_GBP  GBP Curncy  GBPEUR Curncy  \\\n",
       "Date                                                                    \n",
       "2015-01-01        32.818125      31.737054    1.516177       1.336264   \n",
       "2015-02-01        38.343255      32.759207    1.533386       1.362371   \n",
       "2015-03-01        37.866261      31.635113    1.503677       1.377568   \n",
       "2015-04-01        40.409377      35.572047    1.512909       1.386036   \n",
       "2015-05-01        42.505052      36.186768    1.543548       1.384014   \n",
       "2015-06-01        41.067582      35.885473    1.552386       1.385524   \n",
       "2015-07-01        36.726002      31.989864    1.545617       1.402913   \n",
       "2015-08-01        31.196110      29.152671    1.545248       1.376957   \n",
       "2015-09-01        31.774855      30.721787    1.527609       1.364657   \n",
       "2015-10-01        32.150956      30.493892    1.533165       1.378817   \n",
       "2015-11-01        30.159048      28.998764    1.523005       1.409255   \n",
       "2015-12-01        25.818625      27.520854    1.506823       1.366814   \n",
       "2016-01-01        22.809737      25.534394    1.399643       1.278787   \n",
       "2016-02-01        24.047857      26.087339    1.394184       1.260416   \n",
       "2016-03-01        28.183023      28.331057    1.411843       1.258433   \n",
       "2016-04-01        30.652665      29.521709    1.413891       1.256627   \n",
       "2016-05-01        33.610150      32.363833    1.417632       1.269523   \n",
       "2016-06-01        35.853479      33.841453    1.392536       1.244714   \n",
       "2016-07-01        34.979696      32.963639    1.330336       1.201241   \n",
       "2016-08-01        35.390659      33.703121    1.332530       1.191674   \n",
       "2016-09-01        35.650483      33.536749    1.325100       1.186100   \n",
       "2016-10-01        40.400342      37.170194    1.271960       1.152440   \n",
       "2016-11-01        36.390691      33.946930    1.293700       1.192923   \n",
       "2016-12-01        42.778165      38.042859    1.283743       1.197404   \n",
       "\n",
       "              UKX Index    SPX Index   SX5E Index  Natural_Gas  GBP_IRS_2y  \\\n",
       "Date                                                                         \n",
       "2015-01-01  6641.765238  1339.503656  2495.664852    13.099203    0.858274   \n",
       "2015-02-01  6746.840476  1356.232800  2536.801725    13.938965    0.952887   \n",
       "2015-03-01  6791.140000  1380.431518  2615.694574    13.821672    0.947216   \n",
       "2015-04-01  6886.650952  1381.813784  2620.084413    13.690119    0.923663   \n",
       "2015-05-01  6864.994500  1360.216539  2568.111738    13.123988    1.009908   \n",
       "2015-06-01  6687.803500  1340.919813  2507.691659    13.023503    1.061182   \n",
       "2015-07-01  6625.526087  1353.218881  2518.626151    13.169073    1.119989   \n",
       "2015-08-01  6402.255000  1311.027051  2434.955959    12.478583    1.085050   \n",
       "2015-09-01  6280.524348  1302.101893  2375.712634    12.595702    1.004023   \n",
       "2015-10-01  6458.806957  1339.915435  2458.240837    12.291281    0.940307   \n",
       "2015-11-01  6398.511500  1357.124134  2446.822967    12.095331    0.994821   \n",
       "2015-12-01  6324.143500  1363.285466  2426.843201    11.423042    1.016488   \n",
       "2016-01-01  6123.662727  1413.405854  2340.610930     9.773593    0.898800   \n",
       "2016-02-01  6171.272222  1434.117439  2327.359810     9.383225    0.757655   \n",
       "2016-03-01  6253.627368  1452.882656  2395.269808     9.068961    0.805940   \n",
       "2016-04-01  6357.307727  1468.004077  2426.290431     9.156711    0.820131   \n",
       "2016-05-01  6305.287143  1461.227431  2356.760057     9.512007    0.805012   \n",
       "2016-06-01  6262.087727  1497.344936  2335.194660    10.313098    0.707955   \n",
       "2016-07-01  6630.369545  1608.590787  2481.757122    10.837997    0.501131   \n",
       "2016-08-01  6658.116818  1603.181916  2513.310834     9.386618    0.425193   \n",
       "2016-09-01  6681.491364  1607.695474  2522.149367    10.132168    0.440250   \n",
       "2016-10-01  6784.034000  1663.042077  2624.813005    12.497445    0.557524   \n",
       "2016-11-01  6618.945455  1649.769763  2519.838212    12.435263    0.646977   \n",
       "2016-12-01  6796.544286  1706.510219  2631.646533    12.452421    0.649512   \n",
       "\n",
       "            GBP_IRS_10y  Nationwide_HPI  \n",
       "Date                                     \n",
       "2015-01-01     1.684729           375.9  \n",
       "2015-02-01     1.856450           375.0  \n",
       "2015-03-01     1.783045           377.9  \n",
       "2015-04-01     1.811432           385.1  \n",
       "2015-05-01     1.971630           389.3  \n",
       "2015-06-01     2.089545           389.1  \n",
       "2015-07-01     2.051104           390.2  \n",
       "2015-08-01     1.958367           389.6  \n",
       "2015-09-01     1.908904           390.2  \n",
       "2015-10-01     1.899183           392.6  \n",
       "2015-11-01     1.897445           391.6  \n",
       "2015-12-01     1.930550           393.0  \n",
       "2016-01-01     1.504270           392.7  \n",
       "2016-02-01     1.332758           392.9  \n",
       "2016-03-01     1.375611           399.5  \n",
       "2016-04-01     1.418518           403.8  \n",
       "2016-05-01     1.354362           407.7  \n",
       "2016-06-01     1.206173           408.9  \n",
       "2016-07-01     1.021027           410.4  \n",
       "2016-08-01     0.936239           411.2  \n",
       "2016-09-01     0.959009           411.0  \n",
       "2016-10-01     1.138360           410.8  \n",
       "2016-11-01     1.316939           408.8  \n",
       "2016-12-01     1.293796           410.7  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[72:96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8b054e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((72, 11), (72,), (24, 11), (24,), (12, 11), (12,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = X[:72], y[:72]\n",
    "X_val, y_val = X[72:96], y[72:96]\n",
    "X_test, y_test = X[96:108], y[96:108]\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "328b0f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-09 15:42:05.708607: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-09 15:42:05.708759: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-09 15:42:05.708815: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-S0KUROGM): /proc/driver/nvidia/version does not exist\n",
      "2022-09-09 15:42:05.709382: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([72, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_x = expand_dims(y_train, -1)\n",
    "y_train_x = expand_dims(y_train_x, -1)\n",
    "\n",
    "y_train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "792ffcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_x = expand_dims(y_val, -1)\n",
    "y_val_x = expand_dims(y_val_x, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f60dfca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([24, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e857c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_x = expand_dims(X_test, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fe8c15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([12, 1, 11])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc718083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([72, 1, 11])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_x = expand_dims(X_train, -2)\n",
    "X_train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44aa619d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([24, 1, 11])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_x = expand_dims(X_val, -2)\n",
    "X_val_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d05957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = ExponentialDecay(initial_learning_rate=0.001, decay_steps= 10_000, decay_rate=0.7)\n",
    "adam = Adam(learning_rate=lr_scheduler)\n",
    "es = EarlyStopping(patience=50, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d68b373f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_1 (Normalizat  (None, None, 11)         23        \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, None, 100)         44800     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 50)                30200     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               5100      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 25)                1275      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 26        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 86,474\n",
      "Trainable params: 86,451\n",
      "Non-trainable params: 23\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "normalizer = Normalization()\n",
    "normalizer.adapt(X_train_x)\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(normalizer)\n",
    "\n",
    "model1.add(LSTM(units = 100, activation='tanh', return_sequences = True))\n",
    "model1.add(LSTM(units = 50 , activation='tanh', return_sequences = False))\n",
    "\n",
    "model1.add(Flatten())\n",
    "\n",
    "model1.add(Dense(100,activation =  'relu'))\n",
    "model1.add(Dense(50,activation =  'relu'))\n",
    "model1.add(Dense(25,activation =  'relu'))\n",
    "\n",
    "model1.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a952a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape = MeanAbsolutePercentageError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a55dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss= mape, optimizer= adam, metrics=['mape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9df43aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "3/3 [==============================] - 3s 270ms/step - loss: 99.9965 - mape: 99.9965 - val_loss: 99.9914 - val_mape: 99.9914\n",
      "Epoch 2/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 99.9828 - mape: 99.9828 - val_loss: 99.9803 - val_mape: 99.9803\n",
      "Epoch 3/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 99.9660 - mape: 99.9660 - val_loss: 99.9664 - val_mape: 99.9664\n",
      "Epoch 4/10000\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 99.9438 - mape: 99.9438 - val_loss: 99.9497 - val_mape: 99.9497\n",
      "Epoch 5/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 99.9143 - mape: 99.9143 - val_loss: 99.9289 - val_mape: 99.9289\n",
      "Epoch 6/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 99.8772 - mape: 99.8772 - val_loss: 99.9013 - val_mape: 99.9013\n",
      "Epoch 7/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 99.8232 - mape: 99.8232 - val_loss: 99.8650 - val_mape: 99.8650\n",
      "Epoch 8/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 99.7558 - mape: 99.7558 - val_loss: 99.8182 - val_mape: 99.8182\n",
      "Epoch 9/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 99.6601 - mape: 99.6601 - val_loss: 99.7578 - val_mape: 99.7578\n",
      "Epoch 10/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 99.5408 - mape: 99.5408 - val_loss: 99.6790 - val_mape: 99.6790\n",
      "Epoch 11/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 99.3583 - mape: 99.3583 - val_loss: 99.5666 - val_mape: 99.5666\n",
      "Epoch 12/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 99.1157 - mape: 99.1157 - val_loss: 99.4109 - val_mape: 99.4109\n",
      "Epoch 13/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 98.7604 - mape: 98.7604 - val_loss: 99.1995 - val_mape: 99.1995\n",
      "Epoch 14/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 98.2670 - mape: 98.2670 - val_loss: 98.9216 - val_mape: 98.9216\n",
      "Epoch 15/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 97.6170 - mape: 97.6170 - val_loss: 98.5676 - val_mape: 98.5676\n",
      "Epoch 16/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 96.7737 - mape: 96.7737 - val_loss: 98.1033 - val_mape: 98.1033\n",
      "Epoch 17/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 95.5340 - mape: 95.5340 - val_loss: 97.5034 - val_mape: 97.5034\n",
      "Epoch 18/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 94.0940 - mape: 94.0940 - val_loss: 96.7314 - val_mape: 96.7314\n",
      "Epoch 19/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 92.2642 - mape: 92.2642 - val_loss: 95.7030 - val_mape: 95.7030\n",
      "Epoch 20/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 89.8213 - mape: 89.8213 - val_loss: 94.3589 - val_mape: 94.3589\n",
      "Epoch 21/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 86.8366 - mape: 86.8366 - val_loss: 92.6394 - val_mape: 92.6394\n",
      "Epoch 22/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 83.0411 - mape: 83.0411 - val_loss: 90.4637 - val_mape: 90.4637\n",
      "Epoch 23/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 78.5539 - mape: 78.5539 - val_loss: 87.7553 - val_mape: 87.7553\n",
      "Epoch 24/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 72.8666 - mape: 72.8666 - val_loss: 84.3349 - val_mape: 84.3349\n",
      "Epoch 25/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 66.0657 - mape: 66.0657 - val_loss: 80.1644 - val_mape: 80.1644\n",
      "Epoch 26/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 58.2956 - mape: 58.2956 - val_loss: 75.2055 - val_mape: 75.2055\n",
      "Epoch 27/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 50.7350 - mape: 50.7350 - val_loss: 69.2663 - val_mape: 69.2663\n",
      "Epoch 28/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 47.7213 - mape: 47.7213 - val_loss: 62.3747 - val_mape: 62.3747\n",
      "Epoch 29/10000\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 44.0741 - mape: 44.0741 - val_loss: 54.5472 - val_mape: 54.5472\n",
      "Epoch 30/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 39.8786 - mape: 39.8786 - val_loss: 45.7066 - val_mape: 45.7066\n",
      "Epoch 31/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 34.9291 - mape: 34.9291 - val_loss: 35.9853 - val_mape: 35.9853\n",
      "Epoch 32/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 28.7527 - mape: 28.7527 - val_loss: 25.6258 - val_mape: 25.6258\n",
      "Epoch 33/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 21.9383 - mape: 21.9383 - val_loss: 13.6215 - val_mape: 13.6215\n",
      "Epoch 34/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 15.2431 - mape: 15.2431 - val_loss: 6.6577 - val_mape: 6.6577\n",
      "Epoch 35/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 17.7075 - mape: 17.7075 - val_loss: 6.6489 - val_mape: 6.6489\n",
      "Epoch 36/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 17.6708 - mape: 17.6708 - val_loss: 6.0700 - val_mape: 6.0700\n",
      "Epoch 37/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 14.7010 - mape: 14.7010 - val_loss: 7.9569 - val_mape: 7.9569\n",
      "Epoch 38/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 12.2517 - mape: 12.2517 - val_loss: 12.7639 - val_mape: 12.7639\n",
      "Epoch 39/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 13.5931 - mape: 13.5931 - val_loss: 12.0694 - val_mape: 12.0694\n",
      "Epoch 40/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 11.9529 - mape: 11.9529 - val_loss: 8.0097 - val_mape: 8.0097\n",
      "Epoch 41/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 11.5053 - mape: 11.5053 - val_loss: 6.8507 - val_mape: 6.8507\n",
      "Epoch 42/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 11.4731 - mape: 11.4731 - val_loss: 8.1795 - val_mape: 8.1795\n",
      "Epoch 43/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 10.2877 - mape: 10.2877 - val_loss: 11.1821 - val_mape: 11.1821\n",
      "Epoch 44/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 10.8926 - mape: 10.8926 - val_loss: 11.3546 - val_mape: 11.3546\n",
      "Epoch 45/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 10.4725 - mape: 10.4725 - val_loss: 9.0138 - val_mape: 9.0138\n",
      "Epoch 46/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 9.5840 - mape: 9.5840 - val_loss: 7.2650 - val_mape: 7.2650\n",
      "Epoch 47/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 9.5597 - mape: 9.5597 - val_loss: 7.6871 - val_mape: 7.6871\n",
      "Epoch 48/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 9.0233 - mape: 9.0233 - val_loss: 9.0975 - val_mape: 9.0975\n",
      "Epoch 49/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 9.1054 - mape: 9.1054 - val_loss: 8.6174 - val_mape: 8.6174\n",
      "Epoch 50/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 8.7238 - mape: 8.7238 - val_loss: 6.9997 - val_mape: 6.9997\n",
      "Epoch 51/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 8.6137 - mape: 8.6137 - val_loss: 6.7920 - val_mape: 6.7920\n",
      "Epoch 52/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 8.3612 - mape: 8.3612 - val_loss: 7.3439 - val_mape: 7.3439\n",
      "Epoch 53/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 7.9324 - mape: 7.9324 - val_loss: 7.1556 - val_mape: 7.1556\n",
      "Epoch 54/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 7.7797 - mape: 7.7797 - val_loss: 7.0552 - val_mape: 7.0552\n",
      "Epoch 55/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 7.6034 - mape: 7.6034 - val_loss: 6.7317 - val_mape: 6.7317\n",
      "Epoch 56/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 7.5142 - mape: 7.5142 - val_loss: 6.5001 - val_mape: 6.5001\n",
      "Epoch 57/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 7.3510 - mape: 7.3510 - val_loss: 7.1471 - val_mape: 7.1471\n",
      "Epoch 58/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 7.3602 - mape: 7.3602 - val_loss: 6.7415 - val_mape: 6.7415\n",
      "Epoch 59/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 6.9967 - mape: 6.9967 - val_loss: 6.1669 - val_mape: 6.1669\n",
      "Epoch 60/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 16ms/step - loss: 6.9138 - mape: 6.9138 - val_loss: 6.3662 - val_mape: 6.3662\n",
      "Epoch 61/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 6.6419 - mape: 6.6419 - val_loss: 6.4257 - val_mape: 6.4257\n",
      "Epoch 62/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 6.4746 - mape: 6.4746 - val_loss: 6.1788 - val_mape: 6.1788\n",
      "Epoch 63/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 6.3480 - mape: 6.3480 - val_loss: 6.4690 - val_mape: 6.4690\n",
      "Epoch 64/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 6.1709 - mape: 6.1709 - val_loss: 6.5160 - val_mape: 6.5160\n",
      "Epoch 65/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 6.0242 - mape: 6.0242 - val_loss: 6.0979 - val_mape: 6.0979\n",
      "Epoch 66/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 5.9136 - mape: 5.9136 - val_loss: 6.3835 - val_mape: 6.3835\n",
      "Epoch 67/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 5.7213 - mape: 5.7213 - val_loss: 6.4547 - val_mape: 6.4547\n",
      "Epoch 68/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 5.5684 - mape: 5.5684 - val_loss: 6.0450 - val_mape: 6.0450\n",
      "Epoch 69/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 5.5861 - mape: 5.5861 - val_loss: 6.3884 - val_mape: 6.3884\n",
      "Epoch 70/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 5.2536 - mape: 5.2536 - val_loss: 6.8861 - val_mape: 6.8861\n",
      "Epoch 71/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 5.1527 - mape: 5.1527 - val_loss: 6.3759 - val_mape: 6.3759\n",
      "Epoch 72/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 5.2030 - mape: 5.2030 - val_loss: 6.4854 - val_mape: 6.4854\n",
      "Epoch 73/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 4.8724 - mape: 4.8724 - val_loss: 7.3681 - val_mape: 7.3681\n",
      "Epoch 74/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 4.9483 - mape: 4.9483 - val_loss: 7.3619 - val_mape: 7.3619\n",
      "Epoch 75/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 4.7512 - mape: 4.7512 - val_loss: 6.4915 - val_mape: 6.4915\n",
      "Epoch 76/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 4.7077 - mape: 4.7077 - val_loss: 6.6667 - val_mape: 6.6667\n",
      "Epoch 77/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 4.4049 - mape: 4.4049 - val_loss: 7.5696 - val_mape: 7.5696\n",
      "Epoch 78/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 4.6027 - mape: 4.6027 - val_loss: 7.0619 - val_mape: 7.0619\n",
      "Epoch 79/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 4.3537 - mape: 4.3537 - val_loss: 6.6122 - val_mape: 6.6122\n",
      "Epoch 80/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 4.1790 - mape: 4.1790 - val_loss: 7.3297 - val_mape: 7.3297\n",
      "Epoch 81/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 4.1345 - mape: 4.1345 - val_loss: 6.9738 - val_mape: 6.9738\n",
      "Epoch 82/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 3.9417 - mape: 3.9417 - val_loss: 6.6983 - val_mape: 6.6983\n",
      "Epoch 83/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 3.8148 - mape: 3.8148 - val_loss: 7.0231 - val_mape: 7.0231\n",
      "Epoch 84/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 3.7630 - mape: 3.7630 - val_loss: 6.8286 - val_mape: 6.8286\n",
      "Epoch 85/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 3.6386 - mape: 3.6386 - val_loss: 6.6398 - val_mape: 6.6398\n",
      "Epoch 86/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 3.5497 - mape: 3.5497 - val_loss: 7.5540 - val_mape: 7.5540\n",
      "Epoch 87/10000\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 3.6772 - mape: 3.6772 - val_loss: 7.3028 - val_mape: 7.3028\n",
      "Epoch 88/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 3.3740 - mape: 3.3740 - val_loss: 6.5572 - val_mape: 6.5572\n",
      "Epoch 89/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 3.2984 - mape: 3.2984 - val_loss: 7.2725 - val_mape: 7.2725\n",
      "Epoch 90/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 3.1990 - mape: 3.1990 - val_loss: 7.2820 - val_mape: 7.2820\n",
      "Epoch 91/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 2.9717 - mape: 2.9717 - val_loss: 6.2730 - val_mape: 6.2730\n",
      "Epoch 92/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 2.9857 - mape: 2.9857 - val_loss: 6.3998 - val_mape: 6.3998\n",
      "Epoch 93/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 2.6830 - mape: 2.6830 - val_loss: 6.8095 - val_mape: 6.8095\n",
      "Epoch 94/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 2.6354 - mape: 2.6354 - val_loss: 6.5305 - val_mape: 6.5305\n",
      "Epoch 95/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 2.4917 - mape: 2.4917 - val_loss: 6.5669 - val_mape: 6.5669\n",
      "Epoch 96/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 2.3684 - mape: 2.3684 - val_loss: 6.4138 - val_mape: 6.4138\n",
      "Epoch 97/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 2.2298 - mape: 2.2298 - val_loss: 6.2396 - val_mape: 6.2396\n",
      "Epoch 98/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 2.1286 - mape: 2.1286 - val_loss: 6.3919 - val_mape: 6.3919\n",
      "Epoch 99/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 2.0396 - mape: 2.0396 - val_loss: 6.1750 - val_mape: 6.1750\n",
      "Epoch 100/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.9849 - mape: 1.9849 - val_loss: 6.1688 - val_mape: 6.1688\n",
      "Epoch 101/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.8818 - mape: 1.8818 - val_loss: 6.1485 - val_mape: 6.1485\n",
      "Epoch 102/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 1.9427 - mape: 1.9427 - val_loss: 5.9130 - val_mape: 5.9130\n",
      "Epoch 103/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.8764 - mape: 1.8764 - val_loss: 6.3927 - val_mape: 6.3927\n",
      "Epoch 104/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.9118 - mape: 1.9118 - val_loss: 6.0275 - val_mape: 6.0275\n",
      "Epoch 105/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 1.7395 - mape: 1.7395 - val_loss: 5.9637 - val_mape: 5.9637\n",
      "Epoch 106/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.7026 - mape: 1.7026 - val_loss: 6.1944 - val_mape: 6.1944\n",
      "Epoch 107/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.6679 - mape: 1.6679 - val_loss: 6.1795 - val_mape: 6.1795\n",
      "Epoch 108/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.6133 - mape: 1.6133 - val_loss: 6.4192 - val_mape: 6.4192\n",
      "Epoch 109/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.6463 - mape: 1.6463 - val_loss: 6.0654 - val_mape: 6.0654\n",
      "Epoch 110/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.6031 - mape: 1.6031 - val_loss: 6.0001 - val_mape: 6.0001\n",
      "Epoch 111/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.5169 - mape: 1.5169 - val_loss: 6.1797 - val_mape: 6.1797\n",
      "Epoch 112/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 1.5199 - mape: 1.5199 - val_loss: 5.8811 - val_mape: 5.8811\n",
      "Epoch 113/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 1.6271 - mape: 1.6271 - val_loss: 6.0615 - val_mape: 6.0615\n",
      "Epoch 114/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 1.4510 - mape: 1.4510 - val_loss: 5.9291 - val_mape: 5.9291\n",
      "Epoch 115/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 1.4219 - mape: 1.4219 - val_loss: 5.8239 - val_mape: 5.8239\n",
      "Epoch 116/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 1.3859 - mape: 1.3859 - val_loss: 5.8729 - val_mape: 5.8729\n",
      "Epoch 117/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 1.4238 - mape: 1.4238 - val_loss: 5.7738 - val_mape: 5.7738\n",
      "Epoch 118/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 1.3518 - mape: 1.3518 - val_loss: 5.9923 - val_mape: 5.9923\n",
      "Epoch 119/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 1.3887 - mape: 1.3887 - val_loss: 5.6468 - val_mape: 5.6468\n",
      "Epoch 120/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 17ms/step - loss: 1.3442 - mape: 1.3442 - val_loss: 5.5808 - val_mape: 5.5808\n",
      "Epoch 121/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.2744 - mape: 1.2744 - val_loss: 5.8266 - val_mape: 5.8266\n",
      "Epoch 122/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 1.4056 - mape: 1.4056 - val_loss: 5.3140 - val_mape: 5.3140\n",
      "Epoch 123/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 1.3416 - mape: 1.3416 - val_loss: 5.1448 - val_mape: 5.1448\n",
      "Epoch 124/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.2727 - mape: 1.2727 - val_loss: 5.3989 - val_mape: 5.3989\n",
      "Epoch 125/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 1.2583 - mape: 1.2583 - val_loss: 4.8290 - val_mape: 4.8290\n",
      "Epoch 126/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.2798 - mape: 1.2798 - val_loss: 5.1136 - val_mape: 5.1136\n",
      "Epoch 127/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 1.2415 - mape: 1.2415 - val_loss: 4.8263 - val_mape: 4.8263\n",
      "Epoch 128/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 1.2301 - mape: 1.2301 - val_loss: 4.5421 - val_mape: 4.5421\n",
      "Epoch 129/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.3975 - mape: 1.3975 - val_loss: 4.5991 - val_mape: 4.5991\n",
      "Epoch 130/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.2180 - mape: 1.2180 - val_loss: 4.8789 - val_mape: 4.8789\n",
      "Epoch 131/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.3575 - mape: 1.3575 - val_loss: 4.5879 - val_mape: 4.5879\n",
      "Epoch 132/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 1.1030 - mape: 1.1030 - val_loss: 4.1735 - val_mape: 4.1735\n",
      "Epoch 133/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.3258 - mape: 1.3258 - val_loss: 4.2129 - val_mape: 4.2129\n",
      "Epoch 134/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.1075 - mape: 1.1075 - val_loss: 4.2530 - val_mape: 4.2530\n",
      "Epoch 135/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.0717 - mape: 1.0717 - val_loss: 4.3225 - val_mape: 4.3225\n",
      "Epoch 136/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 1.0618 - mape: 1.0618 - val_loss: 4.1321 - val_mape: 4.1321\n",
      "Epoch 137/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 1.1814 - mape: 1.1814 - val_loss: 4.0165 - val_mape: 4.0165\n",
      "Epoch 138/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 1.0937 - mape: 1.0937 - val_loss: 4.0857 - val_mape: 4.0857\n",
      "Epoch 139/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.1014 - mape: 1.1014 - val_loss: 4.2072 - val_mape: 4.2072\n",
      "Epoch 140/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 1.0633 - mape: 1.0633 - val_loss: 3.9404 - val_mape: 3.9404\n",
      "Epoch 141/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 1.0581 - mape: 1.0581 - val_loss: 3.8233 - val_mape: 3.8233\n",
      "Epoch 142/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 1.0549 - mape: 1.0549 - val_loss: 4.0116 - val_mape: 4.0116\n",
      "Epoch 143/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.0453 - mape: 1.0453 - val_loss: 3.8365 - val_mape: 3.8365\n",
      "Epoch 144/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.9206 - mape: 0.9206 - val_loss: 3.6296 - val_mape: 3.6296\n",
      "Epoch 145/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.9448 - mape: 0.9448 - val_loss: 3.7752 - val_mape: 3.7752\n",
      "Epoch 146/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 1.0288 - mape: 1.0288 - val_loss: 3.6078 - val_mape: 3.6078\n",
      "Epoch 147/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.9125 - mape: 0.9125 - val_loss: 3.6492 - val_mape: 3.6492\n",
      "Epoch 148/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.8748 - mape: 0.8748 - val_loss: 3.6748 - val_mape: 3.6748\n",
      "Epoch 149/10000\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.8208 - mape: 0.8208 - val_loss: 3.3882 - val_mape: 3.3882\n",
      "Epoch 150/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.9552 - mape: 0.9552 - val_loss: 3.4549 - val_mape: 3.4549\n",
      "Epoch 151/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.8128 - mape: 0.8128 - val_loss: 3.4012 - val_mape: 3.4012\n",
      "Epoch 152/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.8406 - mape: 0.8406 - val_loss: 3.3306 - val_mape: 3.3306\n",
      "Epoch 153/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.7657 - mape: 0.7657 - val_loss: 3.2457 - val_mape: 3.2457\n",
      "Epoch 154/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.9282 - mape: 0.9282 - val_loss: 3.1915 - val_mape: 3.1915\n",
      "Epoch 155/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.8683 - mape: 0.8683 - val_loss: 3.2606 - val_mape: 3.2606\n",
      "Epoch 156/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.7223 - mape: 0.7223 - val_loss: 3.1145 - val_mape: 3.1145\n",
      "Epoch 157/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.7846 - mape: 0.7846 - val_loss: 3.0810 - val_mape: 3.0810\n",
      "Epoch 158/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.7744 - mape: 0.7744 - val_loss: 3.0952 - val_mape: 3.0952\n",
      "Epoch 159/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.7536 - mape: 0.7536 - val_loss: 2.9649 - val_mape: 2.9649\n",
      "Epoch 160/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.7859 - mape: 0.7859 - val_loss: 2.9773 - val_mape: 2.9773\n",
      "Epoch 161/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.8583 - mape: 0.8583 - val_loss: 2.9190 - val_mape: 2.9190\n",
      "Epoch 162/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.7265 - mape: 0.7265 - val_loss: 2.8782 - val_mape: 2.8782\n",
      "Epoch 163/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.7020 - mape: 0.7020 - val_loss: 2.8352 - val_mape: 2.8352\n",
      "Epoch 164/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6569 - mape: 0.6569 - val_loss: 2.7974 - val_mape: 2.7974\n",
      "Epoch 165/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.7366 - mape: 0.7366 - val_loss: 2.7678 - val_mape: 2.7678\n",
      "Epoch 166/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.7565 - mape: 0.7565 - val_loss: 2.7287 - val_mape: 2.7287\n",
      "Epoch 167/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.7444 - mape: 0.7444 - val_loss: 2.7112 - val_mape: 2.7112\n",
      "Epoch 168/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.7871 - mape: 0.7871 - val_loss: 2.6663 - val_mape: 2.6663\n",
      "Epoch 169/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6580 - mape: 0.6580 - val_loss: 2.6322 - val_mape: 2.6322\n",
      "Epoch 170/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6514 - mape: 0.6514 - val_loss: 2.6062 - val_mape: 2.6062\n",
      "Epoch 171/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.8883 - mape: 0.8883 - val_loss: 2.5689 - val_mape: 2.5689\n",
      "Epoch 172/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.8169 - mape: 0.8169 - val_loss: 2.5637 - val_mape: 2.5637\n",
      "Epoch 173/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.7337 - mape: 0.7337 - val_loss: 2.5606 - val_mape: 2.5606\n",
      "Epoch 174/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.8198 - mape: 0.8198 - val_loss: 2.5679 - val_mape: 2.5679\n",
      "Epoch 175/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6175 - mape: 0.6175 - val_loss: 2.5519 - val_mape: 2.5519\n",
      "Epoch 176/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5877 - mape: 0.5877 - val_loss: 2.5466 - val_mape: 2.5466\n",
      "Epoch 177/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6657 - mape: 0.6657 - val_loss: 2.5161 - val_mape: 2.5161\n",
      "Epoch 178/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.7043 - mape: 0.7043 - val_loss: 2.5059 - val_mape: 2.5059\n",
      "Epoch 179/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.7106 - mape: 0.7106 - val_loss: 2.5168 - val_mape: 2.5168\n",
      "Epoch 180/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5286 - mape: 0.5286 - val_loss: 2.5440 - val_mape: 2.5440\n",
      "Epoch 181/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5312 - mape: 0.5312 - val_loss: 2.5394 - val_mape: 2.5394\n",
      "Epoch 182/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5442 - mape: 0.5442 - val_loss: 2.5301 - val_mape: 2.5301\n",
      "Epoch 183/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5884 - mape: 0.5884 - val_loss: 2.4854 - val_mape: 2.4854\n",
      "Epoch 184/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5015 - mape: 0.5015 - val_loss: 2.5080 - val_mape: 2.5080\n",
      "Epoch 185/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6015 - mape: 0.6015 - val_loss: 2.5074 - val_mape: 2.5074\n",
      "Epoch 186/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6809 - mape: 0.6809 - val_loss: 2.5296 - val_mape: 2.5296\n",
      "Epoch 187/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5079 - mape: 0.5079 - val_loss: 2.5885 - val_mape: 2.5885\n",
      "Epoch 188/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6807 - mape: 0.6807 - val_loss: 2.5093 - val_mape: 2.5093\n",
      "Epoch 189/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6252 - mape: 0.6252 - val_loss: 2.5492 - val_mape: 2.5492\n",
      "Epoch 190/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6144 - mape: 0.6144 - val_loss: 2.4453 - val_mape: 2.4453\n",
      "Epoch 191/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.7451 - mape: 0.7451 - val_loss: 2.4266 - val_mape: 2.4266\n",
      "Epoch 192/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6637 - mape: 0.6637 - val_loss: 2.4520 - val_mape: 2.4520\n",
      "Epoch 193/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.8600 - mape: 0.8600 - val_loss: 2.4911 - val_mape: 2.4911\n",
      "Epoch 194/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6047 - mape: 0.6047 - val_loss: 2.5106 - val_mape: 2.5106\n",
      "Epoch 195/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.8094 - mape: 0.8094 - val_loss: 2.5241 - val_mape: 2.5241\n",
      "Epoch 196/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.8072 - mape: 0.8072 - val_loss: 2.6176 - val_mape: 2.6176\n",
      "Epoch 197/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.7370 - mape: 0.7370 - val_loss: 2.5243 - val_mape: 2.5243\n",
      "Epoch 198/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.8284 - mape: 0.8284 - val_loss: 2.5036 - val_mape: 2.5036\n",
      "Epoch 199/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5978 - mape: 0.5978 - val_loss: 2.5860 - val_mape: 2.5860\n",
      "Epoch 200/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.7867 - mape: 0.7867 - val_loss: 2.5244 - val_mape: 2.5244\n",
      "Epoch 201/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6735 - mape: 0.6735 - val_loss: 2.5356 - val_mape: 2.5356\n",
      "Epoch 202/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.7995 - mape: 0.7995 - val_loss: 2.5247 - val_mape: 2.5247\n",
      "Epoch 203/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.8531 - mape: 0.8531 - val_loss: 2.5198 - val_mape: 2.5198\n",
      "Epoch 204/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5487 - mape: 0.5487 - val_loss: 2.4776 - val_mape: 2.4776\n",
      "Epoch 205/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.2149 - mape: 1.2149 - val_loss: 2.4722 - val_mape: 2.4722\n",
      "Epoch 206/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6865 - mape: 0.6865 - val_loss: 2.5414 - val_mape: 2.5414\n",
      "Epoch 207/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.7139 - mape: 0.7139 - val_loss: 2.4973 - val_mape: 2.4973\n",
      "Epoch 208/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5131 - mape: 0.5131 - val_loss: 2.5138 - val_mape: 2.5138\n",
      "Epoch 209/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5242 - mape: 0.5242 - val_loss: 2.5676 - val_mape: 2.5676\n",
      "Epoch 210/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6161 - mape: 0.6161 - val_loss: 2.5384 - val_mape: 2.5384\n",
      "Epoch 211/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5959 - mape: 0.5959 - val_loss: 2.4794 - val_mape: 2.4794\n",
      "Epoch 212/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5353 - mape: 0.5353 - val_loss: 2.4985 - val_mape: 2.4985\n",
      "Epoch 213/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5835 - mape: 0.5835 - val_loss: 2.4274 - val_mape: 2.4274\n",
      "Epoch 214/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6199 - mape: 0.6199 - val_loss: 2.4423 - val_mape: 2.4423\n",
      "Epoch 215/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5476 - mape: 0.5476 - val_loss: 2.4167 - val_mape: 2.4167\n",
      "Epoch 216/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6743 - mape: 0.6743 - val_loss: 2.4344 - val_mape: 2.4344\n",
      "Epoch 217/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6472 - mape: 0.6472 - val_loss: 2.4332 - val_mape: 2.4332\n",
      "Epoch 218/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5891 - mape: 0.5891 - val_loss: 2.4114 - val_mape: 2.4114\n",
      "Epoch 219/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6316 - mape: 0.6316 - val_loss: 2.6663 - val_mape: 2.6663\n",
      "Epoch 220/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.9708 - mape: 0.9708 - val_loss: 2.4308 - val_mape: 2.4308\n",
      "Epoch 221/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5947 - mape: 0.5947 - val_loss: 2.4230 - val_mape: 2.4230\n",
      "Epoch 222/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5434 - mape: 0.5434 - val_loss: 2.4194 - val_mape: 2.4194\n",
      "Epoch 223/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6266 - mape: 0.6266 - val_loss: 2.4302 - val_mape: 2.4302\n",
      "Epoch 224/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5921 - mape: 0.5921 - val_loss: 2.4988 - val_mape: 2.4988\n",
      "Epoch 225/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5479 - mape: 0.5479 - val_loss: 2.4385 - val_mape: 2.4385\n",
      "Epoch 226/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.5614 - mape: 0.5614 - val_loss: 2.4916 - val_mape: 2.4916\n",
      "Epoch 227/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4439 - mape: 0.4439 - val_loss: 2.5069 - val_mape: 2.5069\n",
      "Epoch 228/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4564 - mape: 0.4564 - val_loss: 2.4876 - val_mape: 2.4876\n",
      "Epoch 229/10000\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.4197 - mape: 0.4197 - val_loss: 2.4219 - val_mape: 2.4219\n",
      "Epoch 230/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6245 - mape: 0.6245 - val_loss: 2.5189 - val_mape: 2.5189\n",
      "Epoch 231/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4651 - mape: 0.4651 - val_loss: 2.3907 - val_mape: 2.3907\n",
      "Epoch 232/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3994 - mape: 0.3994 - val_loss: 2.4748 - val_mape: 2.4748\n",
      "Epoch 233/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4454 - mape: 0.4454 - val_loss: 2.4434 - val_mape: 2.4434\n",
      "Epoch 234/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5790 - mape: 0.5790 - val_loss: 2.5121 - val_mape: 2.5121\n",
      "Epoch 235/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.4460 - mape: 0.4460 - val_loss: 2.4924 - val_mape: 2.4924\n",
      "Epoch 236/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4344 - mape: 0.4344 - val_loss: 2.4683 - val_mape: 2.4683\n",
      "Epoch 237/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3618 - mape: 0.3618 - val_loss: 2.4855 - val_mape: 2.4855\n",
      "Epoch 238/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4969 - mape: 0.4969 - val_loss: 2.4624 - val_mape: 2.4624\n",
      "Epoch 239/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.3602 - mape: 0.3602 - val_loss: 2.4721 - val_mape: 2.4721\n",
      "Epoch 240/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 15ms/step - loss: 0.3706 - mape: 0.3706 - val_loss: 2.5089 - val_mape: 2.5089\n",
      "Epoch 241/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4181 - mape: 0.4181 - val_loss: 2.5009 - val_mape: 2.5009\n",
      "Epoch 242/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3953 - mape: 0.3953 - val_loss: 2.4820 - val_mape: 2.4820\n",
      "Epoch 243/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3413 - mape: 0.3413 - val_loss: 2.4384 - val_mape: 2.4384\n",
      "Epoch 244/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3604 - mape: 0.3604 - val_loss: 2.4575 - val_mape: 2.4575\n",
      "Epoch 245/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5594 - mape: 0.5594 - val_loss: 2.3942 - val_mape: 2.3942\n",
      "Epoch 246/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4546 - mape: 0.4546 - val_loss: 2.4291 - val_mape: 2.4291\n",
      "Epoch 247/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4215 - mape: 0.4215 - val_loss: 2.3925 - val_mape: 2.3925\n",
      "Epoch 248/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4996 - mape: 0.4996 - val_loss: 2.4330 - val_mape: 2.4330\n",
      "Epoch 249/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5526 - mape: 0.5526 - val_loss: 2.4604 - val_mape: 2.4604\n",
      "Epoch 250/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4361 - mape: 0.4361 - val_loss: 2.3606 - val_mape: 2.3606\n",
      "Epoch 251/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5185 - mape: 0.5185 - val_loss: 2.4233 - val_mape: 2.4233\n",
      "Epoch 252/10000\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4266 - mape: 0.4266 - val_loss: 2.3333 - val_mape: 2.3333\n",
      "Epoch 253/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5182 - mape: 0.5182 - val_loss: 2.3546 - val_mape: 2.3546\n",
      "Epoch 254/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4157 - mape: 0.4157 - val_loss: 2.3526 - val_mape: 2.3526\n",
      "Epoch 255/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3745 - mape: 0.3745 - val_loss: 2.3322 - val_mape: 2.3322\n",
      "Epoch 256/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4383 - mape: 0.4383 - val_loss: 2.3233 - val_mape: 2.3233\n",
      "Epoch 257/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5036 - mape: 0.5036 - val_loss: 2.3620 - val_mape: 2.3620\n",
      "Epoch 258/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5630 - mape: 0.5630 - val_loss: 2.2779 - val_mape: 2.2779\n",
      "Epoch 259/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5211 - mape: 0.5211 - val_loss: 2.2849 - val_mape: 2.2849\n",
      "Epoch 260/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4452 - mape: 0.4452 - val_loss: 2.2715 - val_mape: 2.2715\n",
      "Epoch 261/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4583 - mape: 0.4583 - val_loss: 2.2660 - val_mape: 2.2660\n",
      "Epoch 262/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4916 - mape: 0.4916 - val_loss: 2.2953 - val_mape: 2.2953\n",
      "Epoch 263/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3937 - mape: 0.3937 - val_loss: 2.2781 - val_mape: 2.2781\n",
      "Epoch 264/10000\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.3952 - mape: 0.3952 - val_loss: 2.3054 - val_mape: 2.3054\n",
      "Epoch 265/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4113 - mape: 0.4113 - val_loss: 2.3130 - val_mape: 2.3130\n",
      "Epoch 266/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5681 - mape: 0.5681 - val_loss: 2.2858 - val_mape: 2.2858\n",
      "Epoch 267/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5011 - mape: 0.5011 - val_loss: 2.2347 - val_mape: 2.2347\n",
      "Epoch 268/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4354 - mape: 0.4354 - val_loss: 2.3189 - val_mape: 2.3189\n",
      "Epoch 269/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4995 - mape: 0.4995 - val_loss: 2.2766 - val_mape: 2.2766\n",
      "Epoch 270/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4503 - mape: 0.4503 - val_loss: 2.3940 - val_mape: 2.3940\n",
      "Epoch 271/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4404 - mape: 0.4404 - val_loss: 2.3862 - val_mape: 2.3862\n",
      "Epoch 272/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4271 - mape: 0.4271 - val_loss: 2.4976 - val_mape: 2.4976\n",
      "Epoch 273/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5101 - mape: 0.5101 - val_loss: 2.4527 - val_mape: 2.4527\n",
      "Epoch 274/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.7032 - mape: 0.7032 - val_loss: 2.6221 - val_mape: 2.6221\n",
      "Epoch 275/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5385 - mape: 0.5385 - val_loss: 2.5288 - val_mape: 2.5288\n",
      "Epoch 276/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3292 - mape: 0.3292 - val_loss: 2.5397 - val_mape: 2.5397\n",
      "Epoch 277/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3517 - mape: 0.3517 - val_loss: 2.5290 - val_mape: 2.5290\n",
      "Epoch 278/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4376 - mape: 0.4376 - val_loss: 2.5529 - val_mape: 2.5529\n",
      "Epoch 279/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3865 - mape: 0.3865 - val_loss: 2.5069 - val_mape: 2.5069\n",
      "Epoch 280/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3858 - mape: 0.3858 - val_loss: 2.5472 - val_mape: 2.5472\n",
      "Epoch 281/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3763 - mape: 0.3763 - val_loss: 2.5248 - val_mape: 2.5248\n",
      "Epoch 282/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3982 - mape: 0.3982 - val_loss: 2.5123 - val_mape: 2.5123\n",
      "Epoch 283/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3304 - mape: 0.3304 - val_loss: 2.4399 - val_mape: 2.4399\n",
      "Epoch 284/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3672 - mape: 0.3672 - val_loss: 2.5388 - val_mape: 2.5388\n",
      "Epoch 285/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4115 - mape: 0.4115 - val_loss: 2.4984 - val_mape: 2.4984\n",
      "Epoch 286/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4011 - mape: 0.4011 - val_loss: 2.4033 - val_mape: 2.4033\n",
      "Epoch 287/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3567 - mape: 0.3567 - val_loss: 2.4449 - val_mape: 2.4449\n",
      "Epoch 288/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3453 - mape: 0.3453 - val_loss: 2.3929 - val_mape: 2.3929\n",
      "Epoch 289/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3226 - mape: 0.3226 - val_loss: 2.3699 - val_mape: 2.3699\n",
      "Epoch 290/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3337 - mape: 0.3337 - val_loss: 2.4925 - val_mape: 2.4925\n",
      "Epoch 291/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5093 - mape: 0.5093 - val_loss: 2.3772 - val_mape: 2.3772\n",
      "Epoch 292/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4531 - mape: 0.4531 - val_loss: 2.3621 - val_mape: 2.3621\n",
      "Epoch 293/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4057 - mape: 0.4057 - val_loss: 2.3934 - val_mape: 2.3934\n",
      "Epoch 294/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5460 - mape: 0.5460 - val_loss: 2.4567 - val_mape: 2.4567\n",
      "Epoch 295/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4161 - mape: 0.4161 - val_loss: 2.4079 - val_mape: 2.4079\n",
      "Epoch 296/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3550 - mape: 0.3550 - val_loss: 2.4836 - val_mape: 2.4836\n",
      "Epoch 297/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3858 - mape: 0.3858 - val_loss: 2.4047 - val_mape: 2.4047\n",
      "Epoch 298/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3545 - mape: 0.3545 - val_loss: 2.5004 - val_mape: 2.5004\n",
      "Epoch 299/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4327 - mape: 0.4327 - val_loss: 2.3549 - val_mape: 2.3549\n",
      "Epoch 300/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4019 - mape: 0.4019 - val_loss: 2.4317 - val_mape: 2.4317\n",
      "Epoch 301/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5604 - mape: 0.5604 - val_loss: 2.3603 - val_mape: 2.3603\n",
      "Epoch 302/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4997 - mape: 0.4997 - val_loss: 2.3299 - val_mape: 2.3299\n",
      "Epoch 303/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5151 - mape: 0.5151 - val_loss: 2.3708 - val_mape: 2.3708\n",
      "Epoch 304/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3654 - mape: 0.3654 - val_loss: 2.4249 - val_mape: 2.4249\n",
      "Epoch 305/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3706 - mape: 0.3706 - val_loss: 2.3219 - val_mape: 2.3219\n",
      "Epoch 306/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5918 - mape: 0.5918 - val_loss: 2.3348 - val_mape: 2.3348\n",
      "Epoch 307/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4855 - mape: 0.4855 - val_loss: 2.4276 - val_mape: 2.4276\n",
      "Epoch 308/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4228 - mape: 0.4228 - val_loss: 2.2814 - val_mape: 2.2814\n",
      "Epoch 309/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3811 - mape: 0.3811 - val_loss: 2.3147 - val_mape: 2.3147\n",
      "Epoch 310/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3586 - mape: 0.3586 - val_loss: 2.2289 - val_mape: 2.2289\n",
      "Epoch 311/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3129 - mape: 0.3129 - val_loss: 2.1924 - val_mape: 2.1924\n",
      "Epoch 312/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3275 - mape: 0.3275 - val_loss: 2.1815 - val_mape: 2.1815\n",
      "Epoch 313/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3677 - mape: 0.3677 - val_loss: 2.2342 - val_mape: 2.2342\n",
      "Epoch 314/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3242 - mape: 0.3242 - val_loss: 2.2320 - val_mape: 2.2320\n",
      "Epoch 315/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3659 - mape: 0.3659 - val_loss: 2.2694 - val_mape: 2.2694\n",
      "Epoch 316/10000\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.4536 - mape: 0.4536 - val_loss: 2.2462 - val_mape: 2.2462\n",
      "Epoch 317/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3200 - mape: 0.3200 - val_loss: 2.2668 - val_mape: 2.2668\n",
      "Epoch 318/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3036 - mape: 0.3036 - val_loss: 2.2052 - val_mape: 2.2052\n",
      "Epoch 319/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3429 - mape: 0.3429 - val_loss: 2.2453 - val_mape: 2.2453\n",
      "Epoch 320/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3478 - mape: 0.3478 - val_loss: 2.2530 - val_mape: 2.2530\n",
      "Epoch 321/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4206 - mape: 0.4206 - val_loss: 2.2792 - val_mape: 2.2792\n",
      "Epoch 322/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4653 - mape: 0.4653 - val_loss: 2.4099 - val_mape: 2.4099\n",
      "Epoch 323/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3938 - mape: 0.3938 - val_loss: 2.2870 - val_mape: 2.2870\n",
      "Epoch 324/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6171 - mape: 0.6171 - val_loss: 2.3055 - val_mape: 2.3055\n",
      "Epoch 325/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4220 - mape: 0.4220 - val_loss: 2.4094 - val_mape: 2.4094\n",
      "Epoch 326/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4712 - mape: 0.4712 - val_loss: 2.2405 - val_mape: 2.2405\n",
      "Epoch 327/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4565 - mape: 0.4565 - val_loss: 2.2786 - val_mape: 2.2786\n",
      "Epoch 328/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4825 - mape: 0.4825 - val_loss: 2.2325 - val_mape: 2.2325\n",
      "Epoch 329/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3098 - mape: 0.3098 - val_loss: 2.2770 - val_mape: 2.2770\n",
      "Epoch 330/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4031 - mape: 0.4031 - val_loss: 2.3220 - val_mape: 2.3220\n",
      "Epoch 331/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4356 - mape: 0.4356 - val_loss: 2.3125 - val_mape: 2.3125\n",
      "Epoch 332/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4595 - mape: 0.4595 - val_loss: 2.3117 - val_mape: 2.3117\n",
      "Epoch 333/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4637 - mape: 0.4637 - val_loss: 2.4816 - val_mape: 2.4816\n",
      "Epoch 334/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4583 - mape: 0.4583 - val_loss: 2.3466 - val_mape: 2.3466\n",
      "Epoch 335/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4685 - mape: 0.4685 - val_loss: 2.4909 - val_mape: 2.4909\n",
      "Epoch 336/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4564 - mape: 0.4564 - val_loss: 2.3844 - val_mape: 2.3844\n",
      "Epoch 337/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4960 - mape: 0.4960 - val_loss: 2.5380 - val_mape: 2.5380\n",
      "Epoch 338/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6565 - mape: 0.6565 - val_loss: 2.3686 - val_mape: 2.3686\n",
      "Epoch 339/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6296 - mape: 0.6296 - val_loss: 2.5194 - val_mape: 2.5194\n",
      "Epoch 340/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4224 - mape: 0.4224 - val_loss: 2.5147 - val_mape: 2.5147\n",
      "Epoch 341/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4597 - mape: 0.4597 - val_loss: 2.4578 - val_mape: 2.4578\n",
      "Epoch 342/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3666 - mape: 0.3666 - val_loss: 2.5381 - val_mape: 2.5381\n",
      "Epoch 343/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.2946 - mape: 0.2946 - val_loss: 2.4416 - val_mape: 2.4416\n",
      "Epoch 344/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3130 - mape: 0.3130 - val_loss: 2.4791 - val_mape: 2.4791\n",
      "Epoch 345/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3044 - mape: 0.3044 - val_loss: 2.4864 - val_mape: 2.4864\n",
      "Epoch 346/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3157 - mape: 0.3157 - val_loss: 2.4220 - val_mape: 2.4220\n",
      "Epoch 347/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3336 - mape: 0.3336 - val_loss: 2.3818 - val_mape: 2.3818\n",
      "Epoch 348/10000\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3052 - mape: 0.3052 - val_loss: 2.3130 - val_mape: 2.3130\n",
      "Epoch 349/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.2940 - mape: 0.2940 - val_loss: 2.3611 - val_mape: 2.3611\n",
      "Epoch 350/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3360 - mape: 0.3360 - val_loss: 2.2261 - val_mape: 2.2261\n",
      "Epoch 351/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4002 - mape: 0.4002 - val_loss: 2.2430 - val_mape: 2.2430\n",
      "Epoch 352/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3782 - mape: 0.3782 - val_loss: 2.2385 - val_mape: 2.2385\n",
      "Epoch 353/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3653 - mape: 0.3653 - val_loss: 2.2714 - val_mape: 2.2714\n",
      "Epoch 354/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3083 - mape: 0.3083 - val_loss: 2.3252 - val_mape: 2.3252\n",
      "Epoch 355/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.2923 - mape: 0.2923 - val_loss: 2.2937 - val_mape: 2.2937\n",
      "Epoch 356/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5254 - mape: 0.5254 - val_loss: 2.3906 - val_mape: 2.3906\n",
      "Epoch 357/10000\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.2902 - mape: 0.2902 - val_loss: 2.2755 - val_mape: 2.2755\n",
      "Epoch 358/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4767 - mape: 0.4767 - val_loss: 2.3942 - val_mape: 2.3942\n",
      "Epoch 359/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3531 - mape: 0.3531 - val_loss: 2.3045 - val_mape: 2.3045\n",
      "Epoch 360/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3539 - mape: 0.3539 - val_loss: 2.4311 - val_mape: 2.4311\n",
      "Epoch 361/10000\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4359 - mape: 0.4359 - val_loss: 2.3050 - val_mape: 2.3050\n",
      "Epoch 362/10000\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4502 - mape: 0.4502 - val_loss: 2.3499 - val_mape: 2.3499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd34a33f550>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train_x, y_train,batch_size = 32, validation_data=(X_val_x, y_val), epochs=10000, callbacks = (es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "640f6ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 458ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[265.6504 ],\n",
       "       [266.59818],\n",
       "       [265.92868],\n",
       "       [267.57196],\n",
       "       [268.58353],\n",
       "       [268.35617],\n",
       "       [268.26636],\n",
       "       [268.0761 ],\n",
       "       [268.89023],\n",
       "       [269.05786],\n",
       "       [269.9922 ],\n",
       "       [271.66568]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.predict(X_test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9abdd57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "test_predictions = list(model1.predict(X_test_x))\n",
    "test_results = pd.DataFrame(columns = ['test_predictions', 'test_actual'])\n",
    "\n",
    "#train_predictions\n",
    "\n",
    "test_results['date'] = list(y_test.index)\n",
    "test_results['test_predictions'] = [x[0] for x in test_predictions]\n",
    "test_results['test_actual'] = list(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe9ad0ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_predictions</th>\n",
       "      <th>test_actual</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>265.650391</td>\n",
       "      <td>265.5</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>266.598175</td>\n",
       "      <td>268.4</td>\n",
       "      <td>2017-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>265.928680</td>\n",
       "      <td>269.3</td>\n",
       "      <td>2017-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>267.571960</td>\n",
       "      <td>270.6</td>\n",
       "      <td>2017-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>268.583527</td>\n",
       "      <td>271.7</td>\n",
       "      <td>2017-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>268.356171</td>\n",
       "      <td>272.3</td>\n",
       "      <td>2017-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>268.266357</td>\n",
       "      <td>272.9</td>\n",
       "      <td>2017-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>268.076111</td>\n",
       "      <td>274.7</td>\n",
       "      <td>2017-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>268.890228</td>\n",
       "      <td>275.1</td>\n",
       "      <td>2017-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>269.057861</td>\n",
       "      <td>275.3</td>\n",
       "      <td>2017-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>269.992188</td>\n",
       "      <td>275.8</td>\n",
       "      <td>2017-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>271.665680</td>\n",
       "      <td>278.1</td>\n",
       "      <td>2017-12-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    test_predictions  test_actual       date\n",
       "0         265.650391        265.5 2017-01-01\n",
       "1         266.598175        268.4 2017-02-01\n",
       "2         265.928680        269.3 2017-03-01\n",
       "3         267.571960        270.6 2017-04-01\n",
       "4         268.583527        271.7 2017-05-01\n",
       "5         268.356171        272.3 2017-06-01\n",
       "6         268.266357        272.9 2017-07-01\n",
       "7         268.076111        274.7 2017-08-01\n",
       "8         268.890228        275.1 2017-09-01\n",
       "9         269.057861        275.3 2017-10-01\n",
       "10        269.992188        275.8 2017-11-01\n",
       "11        271.665680        278.1 2017-12-01"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2767a48b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_results.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "688dfdcd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig1 = plt.plot(y)\n",
    "fig1 = plt.plot(test_results['test_predictions'], label='prediction')\n",
    "fig1 = plt.plot(test_results['test_actual'], label='actual')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd81ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252861c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
